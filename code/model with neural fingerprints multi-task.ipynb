{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "torch.manual_seed(8) # for reproduce\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "# then import my own modules\n",
    "from Featurization import save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class LigandBased(nn.Module):\n",
    "\n",
    "    def __init__(self, radius, input_feature_dim, input_bond_dim,\\\n",
    "            fingerprint_dim, output_units_num, p_dropout=0.0):\n",
    "        super(LigandBased, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=p_dropout)\n",
    "        self.atom_fc = nn.Linear(input_feature_dim, fingerprint_dim)        \n",
    "        self.neighbor_fc = nn.Linear(input_feature_dim+input_bond_dim, fingerprint_dim)\n",
    "        self.atom_GRU = nn.ModuleList([nn.GRUCell(fingerprint_dim, fingerprint_dim) for r in range(radius)])\n",
    "        self.hidden = nn.Linear(fingerprint_dim, fingerprint_dim*2)\n",
    "        self.output = nn.Linear(fingerprint_dim*2, output_units_num) \n",
    "        self.radius = radius\n",
    "\n",
    "    def forward(self, atom_list, bond_list, atom_degree_list, bond_degree_list, atom_mask):\n",
    "        atom_mask = atom_mask.unsqueeze(2)\n",
    "        batch_size,mol_length,num_atom_feat = atom_list.size()\n",
    "        atom_feature = F.relu(self.atom_fc(atom_list)) * atom_mask\n",
    "\n",
    "        bond_neighbor = [bond_list[i][bond_degree_list[i]] for i in range(batch_size)]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        atom_neighbor = [atom_list[i][atom_degree_list[i]] for i in range(batch_size)]\n",
    "        atom_neighbor = torch.stack(atom_neighbor, dim=0)\n",
    "        # then catenate them\n",
    "        neighbor_feature = torch.cat([atom_neighbor, bond_neighbor],dim=-1)\n",
    "#         print(neighbor_feature.shape, neighbor_feature[0][0])\n",
    "        max_features = torch.max(neighbor_feature,dim=-2)[0]\n",
    "        max_features = F.relu(self.neighbor_fc(max_features))\n",
    "#         print(max_features.shape, max_features[0])\n",
    "        batch_size, mol_length, fingerprint_dim = atom_feature.shape\n",
    "        atom_feature_reshape = atom_feature.view(batch_size*mol_length, fingerprint_dim)\n",
    "        max_features_reshape = max_features.view(batch_size*mol_length, fingerprint_dim)\n",
    "        atom_feature_GRU = self.atom_GRU[0](max_features_reshape, atom_feature_reshape)\n",
    "        atom_feature = atom_feature_GRU.view(batch_size, mol_length, fingerprint_dim) * atom_mask        \n",
    "\n",
    "        for d in range(self.radius-1):\n",
    "            neighbor_feature = [atom_feature[i][atom_degree_list[i]] for i in range(batch_size)]\n",
    "            # neighbor_feature is a list of 3D tensor, so we need to stack them into a 4D tensor first\n",
    "            neighbor_feature = torch.stack(neighbor_feature, dim=0)\n",
    "            # then max-pooling \n",
    "            max_features = torch.max(neighbor_feature,dim=-2)[0]\n",
    "\n",
    "            atom_feature_reshape = atom_feature.view(batch_size*mol_length, fingerprint_dim)\n",
    "            max_features_reshape = max_features.view(batch_size*mol_length, fingerprint_dim)\n",
    "            atom_feature_GRU = self.atom_GRU[d+1](max_features_reshape, atom_feature_reshape)\n",
    "            atom_feature = atom_feature_GRU.view(batch_size, mol_length, fingerprint_dim) * atom_mask\n",
    "        mol_feature = torch.sum(atom_feature,-2)\n",
    "        hidden_feature = self.hidden(self.dropout(F.relu(mol_feature)))\n",
    "        mol_prediction = self.output(self.dropout(F.relu(hidden_feature)))\n",
    "        return mol_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "from IPython.display import SVG, display\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kinase count: 392\n",
      "similes count: 2140\n",
      "number of all smiles:  2140\n",
      "number of successfully processed smiles:  2140\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAC/CAYAAAB+KF5fAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAF9RJREFUeJzt3X1QFOcdB/AvJ8chLzVgD5LR8TVwEBFQJ1a0TXVgzEkN4ASCcSRDYzAapxMhjWITZtqoozVXR1pMophY6zCM1UpOx/FlNNY/qk0bUYnxogPBtI45WCHlVfbuYPvH5TZuDuFY4e6A72fGGXn2d8ezT8z39tndZy9AkiQJREQ0IBpfd4CIaDhieBIRqcDwJCJSgeFJRKQCw5OISAWGJxGRCgxPIiIVGJ5ERCowPImIVGB4EhGpwPAkIlKB4UlEpALDk4hIhUBfd2AwfPttB3p6RtbDocaPD0NTU7uvuzFscLw8x7FS0mgCEBEROuDXjYjw7OmRRlx4AhiR+zSUOF6e41g9Ok7biYhUYHgSEanA8CQiUoHhSUSkwoi4YESDx9EDiHZHnzU6bSAC+bFLoxzDkxREuwP/tjT0WfN0fDQCdfynQ6Mbjx+IiFRgeBIRqcDwJCJSod/w/Pzzz7Fu3TosWrQIiYmJWLBgAVatWoXq6mq32urqarz44otISkrCggULsGXLFty/f9+tzmaz4d1338VPf/pTJCYm4oUXXsClS5cGZ4+IiLyg3/D873//i+7ubuTk5KCkpASrVq1Cc3MzVq5ciX/84x9yncViQX5+PkRRRHFxMbKzs3Ho0CEUFha6vWdxcTEOHDiAjIwMvPXWW9BoNCgoKMCVK1cGd++IiIZIgCRJA17kev/+faSlpSEhIQF79uwBABQUFODmzZs4efIkQkOdi+wPHz6Mt99+G3/+85+RkpICAKipqUFOTg42bdqE/Px8AIAoili6dCmioqJQUVEx4J1oamofcWt19fpwCEKb139vh+jZ1fZQP7va7qvxGo44VkoaTQDGjw8b+OvU/LKxY8ciMjISra2tAID29nZcvHgRWVlZcnACQGZmJkJCQnDy5Em57dSpU9BqtcjJyZHbdDodsrOzcfnyZTQ2NqrpEhGRV3kcnu3t7WhubsZXX32FnTt34tatW/LR5M2bN+FwOJCQkKB4TVBQEOLj42GxWOQ2i8WCqVOnKkIWABITEyFJkqKWiMhfeTz3+s1vfoPTp08DALRaLZYvX441a9YAAARBAADo9Xq31+n1ely9elX+WRAEREdH91oHQNWRp5pD7uFArw/3+u+UmjsRHhbcZ01IiA76yBAv9chzvhiv4Ypj9eg8Ds9169YhNzcXVqsVZrMZNpsNdrsdQUFB6OrqAuA80vwhnU4nbweArq4uaLXaXusA5/nPgRrp5zy9uWSyU3Sgrb2r75pOEUJ396P/skHE83ie41gpqT3n6XF4GgwGGAwGAEBGRgaef/55bNq0CX/84x8RHOw8UrHZbG6vE0VR3g4AwcHBsNvtvdYB34cofY9LJon8j6pjFa1Wi9TUVJw5cwZdXV3ylNs1fX+QIAiIioqSf9br9b1OzV2vfbCWiMhfqZ7odXV1QZIkdHR0IDY2FoGBgbh+/bqixmazwWKxID4+Xm6Li4tDfX09Ojo6FLXXrl2TtxMR+bt+w7O5udmtrb29HadPn8YTTzyB8ePHIzw8HCkpKTCbzYpQNJvN6OzshNFolNuMRiPsdjsOHz4st9lsNhw9ehSzZ8/u9WISEZG/6fck2fr166HT6TBr1izo9Xp88803OHr0KKxWK3bu3CnXFRYWYvny5cjLy0NOTg6sViv279+PZ555BvPnz5frkpKSYDQaYTKZIAgCJk2ahKqqKty9exfbtm0bmr0kIhpk/a4wOnLkCMxmM2pra9Ha2orw8HAkJyfj5Zdfxty5cxW1n332GUwmE27cuIGwsDCkp6ejqKgIISHK21pEUcSuXbtw/PhxtLS0wGAwoKioSBGyAzHSr7Z7c9UPVxiNfBwrJbVX21Utz/Q3DE+GJwPBcxwrJa8uzyQiGu0YnkREKjA8iYhU8K8TVzSkPFnmOcJOHRMNGYbnKOLJMs+kWPeHuxCRO07biYhUYHgSEanA8CQiUoHhSUSkAsOTiEgFhicRkQoMTyIiFRieREQqMDyJiFRgeBIRqcDwJCJSgeFJRKQCw5OISAWGJxGRCgxPIiIVGJ5ERCowPImIVGB4EhGpwPAkIlKB4UlEpALDk4hIBYYnEZEKDE8iIhX6/d72mpoaVFVV4dNPP8Xdu3fx2GOPYdasWVi/fj0mT56sqK2ursa7776LGzduICwsDEuWLMEbb7yBsWPHKupsNhtKS0thNpvR2tqKuLg4FBYWIiUlZXD3bhQJ0ASgQ3T0WdMjeakzRKNAv+G5b98+VFdXw2g0wmAwQBAEVFRUICsrC0eOHMH06dMBABaLBfn5+XjyySdRXFwMq9WKjz76CHfu3MEHH3ygeM/i4mKcOXMGL730EiZPnoyqqioUFBTg4MGDmDVr1tDs6Qgn2rtx7ZbQZ01SrN5LvSEa+foNz/z8fJhMJgQFBclt6enpeO6551BeXo7t27cDAHbu3InHHnsMBw8eRGhoKABg4sSJePvtt3Hp0iX5qLKmpgYnTpzApk2bkJ+fDwDIysrC0qVLYTKZUFFRMdj7SEQ06Po95zl79mxFcALAlClTEBMTg7q6OgBAe3s7Ll68iKysLDk4ASAzMxMhISE4efKk3Hbq1ClotVrk5OTIbTqdDtnZ2bh8+TIaGxsfeaeIiIaaqgtGkiTh3r17iIiIAADcvHkTDocDCQkJirqgoCDEx8fDYrHIbRaLBVOnTlWELAAkJiZCkiRFLRGRv+p32t6bY8eOoaGhAYWFhQAAQXCea9Pr3c+p6fV6XL16Vf5ZEARER0f3WgdA1ZHn+PFhA37NcKDXhwMApOZOhIcF91mr1QZ6rSYkRAd9ZEifNb7gGi/qH8fq0Q04POvq6vDOO+9gzpw5yMzMBAB0dXUBgNv0HnBOyV3bXbVarbbXOgAQRXGgXUJTUzt6RtilZL0+HILQBgDoFB1oa+/qs95u915NZ6cIobu7zxpve3C8qG8cKyWNJkDVAdiApu2CIODVV1/FuHHjUFpaCo3G+fLgYOeRis1mc3uNKIrydlet3W7vtQ74PkSJiPyZx0eebW1tKCgoQFtbGyorKxVTdNffXdP3BwmCgKioKEVtb1Nz12sfrCUi8lceHXmKoog1a9bg9u3b2LNnD6ZNm6bYHhsbi8DAQFy/fl3RbrPZYLFYEB8fL7fFxcWhvr4eHR0ditpr167J24mI/F2/4dnd3Y3169fj6tWrKC0tRXJysltNeHg4UlJSYDabFaFoNpvR2dkJo9EotxmNRtjtdhw+fFhus9lsOHr0KGbPnt3rxSQiIn/T77R9+/bt+OSTT7Bo0SL873//g9lslreFhoYiLS0NAFBYWIjly5cjLy8POTk5sFqt2L9/P5555hnMnz9ffk1SUhKMRiNMJhMEQcCkSZNQVVWFu3fvYtu2bUOwi0REg6/f8Pzyyy8BAOfPn8f58+cV2yZMmCCH54wZM7B//36YTCZs27YNYWFheOGFF1BUVOT2njt27MCuXbtgNpvR0tICg8GAvXv3Ys6cOYOxTzTEPFlHr9MGIpCPnaERLECSpGF/j89Iv1WpQ3Tg35aGPuuTYvUerW33Vs3T8dEI1am6jVgV3n7jOY6VklduVSIiIieGJxGRCgxPIiIVGJ5ERCowPImIVGB4EhGpwPAkIlKB4UlEpALDk4hIBe8tAaFRhUs4aaRjeNKQ8OSrkOfOeByi/eHLahmu5M8YnuQz/QXs0/HRCPTi+niigeDnOhGRCgxPIiIVGJ5ERCowPImIVGB4EhGpwPAkIlKB4UlEpALDk4hIBd6B7GOOHkC0uy9jlJo70fnd8sYR9t12RCMCw9PHRHvv34wZHhaMtvYuAM5vqyQi/8JpOxGRCgxPIiIVGJ5ERCowPImIVGB4EhGpwPAkIlLBo/BsbGyEyWRCXl4eZs2aBYPBgE8//bTX2nPnzmHZsmWYOXMmFi5ciLKyMjgc7vcxtra2oqSkBPPmzUNycjJeeuklWCyWR9sbIiIv8Sg86+vrUV5ejoaGBhgMhofWXbhwAevWrcO4ceNQUlKCtLQ07N69G9u2bVPU9fT0YPXq1Thx4gRWrlyJN998E01NTcjLy8N//vOfR9sjIiIv8Ogm+RkzZuCf//wnIiIicPbsWaxbt67Xuh07duCpp57Chx9+iDFjxgAAQkNDsXfvXuTl5WHKlCkAgFOnTuHKlSvYvXs30tLSAABLlizBs88+i7KyMuzYsWMQdo2IaOh4dOQZFhaGiIiIPmtqa2tRW1uL3NxcOTgBYMWKFejp6cGZM2fkttOnTyMqKgqpqalyW2RkJJYsWYKzZ8/CbrcPdD+IiLxq0C4Y3bhxAwCQkJCgaI+Ojsbjjz8ubwcAi8WCGTNmICAgQFE7c+ZMdHR0jJipu6MH6BAdff7hunWi4WnQ1rYLgvNbEPV693XYer0ejY2Nitp58+a51UVFRQFwXqCaPn26x797/PiwgXbXKxqbO/HlV0191hgmRyA8LLjXba52rTbwoTUuI7EmJEQHfWRIn+/xIL0+3OPa0Y5j9egGLTy7upwPsQgKCnLbptPpcP/+fUVtb3WuNtd7eaqpqR09fngI1yk65Id7PIzd3nvNgw8GeViNJ+8znGs6O0UI3d19voeLXh8OQWjzqHa041gpaTQBqg7ABm3aHhzsPIKw2Wxu20RRlLe7anurc7U9WEtE5I8GLTxd03XX9P1BgiDIU3JX7YPTeBdX24O1RET+aNDCMz4+HgBw/fp1RXtDQwOsVqu8HQDi4uLwxRdfQJKUU+2amhqEhIRg0qRJg9UtIqIhMWjhGRMTg2nTpuHQoUPofuA8VWVlJTQaDRYvXiy3GY1GNDY24ty5c3Jbc3MzTp06hdTUVGi12sHqFg1jAZqAfu9WcPT4upc0Wnl8wei9994DANTV1QEAzGYzLl++jB/96EdYuXIlAGDDhg1Yu3YtVq1ahfT0dNy6dQsVFRXIzc3F1KlT5fd69tlnkZycjA0bNuDll19GREQEKisr0dPTg1/96leDuX80jIn2bly75X4a6EFPx0cjUMcvRCDv8/hfXWlpqeLnv/3tbwCACRMmyOG5aNEilJWVoaysDJs3b0ZkZCTWrl2L1157TfHaMWPGYO/evdixYwcOHjwIURQxc+ZM/P73v8fkyZMfdZ+IiIacx+F58+ZNj+rS0tLkJZd9GTduHLZu3YqtW7d62gUiIr/BR9IREanA8CQiUoHhSUSkAi9TquTocX7nel/8cMUoEQ0ShqdKot2Bf1sa+qxJinV/SAoRjQycthMRqcDwJCJSgeFJRKQCw5OISAWGJxGRCgxPIiIVGJ5ERCowPImIVGB4EhGpwBVGNOJ5spRWpw1EIA8laAAYnjTiebKUlk+kp4HiZy0RkQoMTyIiFRieREQqMDyJiFRgeBIRqcDwJCJSgeFJRKQCb2yjYS1AE4AO0QGpuROdYu83wvO7pGgojMrw5IqTkUO0d+PaLQHhYcFoa+/qtYbfJUVDYVSGJ1ecENGjYjo8hGs6+DCcChKNbgzPh3BNBx+GU8GRpb8PS4CnckjJZ+Fps9lQWloKs9mM1tZWxMXFobCwECkpKb7qEo1i/X1YAjyVQ0o++xwtLi7GgQMHkJGRgbfeegsajQYFBQW4cuWKr7pEROQxn4RnTU0NTpw4gV//+tfYsGEDcnNzceDAATzxxBMwmUy+6BIR0YD4JDxPnToFrVaLnJwcuU2n0yE7OxuXL19GY2OjL7pFROQxn5zAsVgsmDp1KkJDQxXtiYmJkCQJFosFUVFRHr+fRhMwoN8fOEaDkGDtI9UMxnv0VTNWF4huh9Yrv8tfawbyHg+O11D2t79/a909gM3R3WdNUOAYjOnnsGWw3udhBvr/zEimdiwCJEny+k03S5cuRXR0ND788ENFe21tLX7xi19gy5YtiqNSIiJ/45Npe1dXF7Ra9095nU4HABBF0dtdIiIaEJ+EZ3BwMOx2u1u7KzRdIUpE5K98Ep56vb7Xi0KC4LzPbiDnO4mIfMEn4RkXF4f6+np0dHQo2q9duyZvJyLyZz4JT6PRCLvdjsOHD8ttNpsNR48exezZsxEdHe2LbhERecwntyolJSXBaDTCZDJBEARMmjQJVVVVuHv3LrZt2+aLLhERDYhPblUCnBeHdu3ahePHj6OlpQUGgwFFRUWYP3++L7pDRDQgPgtPIqLhjA/YIiJSgeFJRKQCw9MHampq8Lvf/Q7p6elITk7GwoULUVhYiK+//tqttrq6Gi+++CKSkpKwYMECbNmyBffv3/dBr/1HeXk5DAYDMjMz3bZxvJxqamqwevVqPP3005g1axYyMjJw9OhRRc25c+ewbNkyzJw5EwsXLkRZWRkcjr4fCE3f45NdfWDfvn2orq6G0WiEwWCAIAioqKhAVlYWjhw5gunTpwNwPkAlPz8fTz75JIqLi2G1WvHRRx/hzp07+OCDD3y8F74hCALef/99hISEuG3jeDlduHAB69atw9y5c/H6668jMDAQt2/fxjfffONWM2/ePJSUlODWrVvYvXs3vv32W5SUlPiw98OIRF53+fJlSRRFRVt9fb2UkJAgbdy4UW575ZVXpJ/97GdSe3u73PbXv/5Vio2NlS5evOi1/vqTjRs3Snl5edLKlSuljIwMxTaOlyS1trZKKSkp0ubNm/usS09Pl5YtWyY5HA65befOnVJcXJxUX18/xL0cGTht94HZs2cjKChI0TZlyhTExMSgrq4OANDe3o6LFy8iKytL8ei+zMxMhISE4OTJk17tsz+oqanBsWPHsGnTJrdtHC+n48ePo7W1Fa+//joA57hIP7ihpra2FrW1tcjNzcWYMWPk9hUrVqCnpwdnzpzxap+HK4ann5AkCffu3UNERAQA4ObNm3A4HEhISFDUBQUFIT4+HhaLxRfd9BlJkrB582ZkZWUhPj7ebTvHy+nSpUuYNm0aLly4gJ///OeYM2cO5s6dC5PJhO5u5/NBb9y4AQBuYxUdHY3HH39c3k59Y3j6iWPHjqGhoQFLliwB8P1DUvR692/pfNiDVUayjz/+GLW1tVi/fn2v2zleTl9//TWsViuKi4uxbNky/OlPf0JaWhrKy8uxfft2AByrwcILRn6grq4O77zzDubMmSNfQe7q6gIAt+k94Hxkn2v7aNDe3o4//OEPWL169UOfuMXxcurs7ERLSwveeOMNrF69GgCwePFidHZ2orKyEmvXru13rEbj3Qlq8MjTxwRBwKuvvopx48ahtLQUGo3zP0lwcDAA5wNTfkgURXn7aPD+++9Dq9Xil7/85UNrOF5Orv1cunSpov25556D3W7H559/zrEaJAxPH2pra0NBQQHa2tqwb98+xTTK9XfXFOtBgiCMmmeeNjY24sCBA1ixYgXu3buHO3fu4M6dOxBFEXa7HXfu3EFLSwvH6zuucfjxj3+saHf9zLEaPAxPHxFFEWvWrMHt27exZ88eTJs2TbE9NjYWgYGBuH79uqLdZrPBYrH0etFkJGpqaoLdbofJZEJqaqr859q1a6irq0NqairKy8s5Xt+ZMWMGAKChoUHRbrVaAQCRkZHyWPxwrBoaGmC1WkfNWD0qhqcPdHd3Y/369bh69SpKS0uRnJzsVhMeHo6UlBSYzWbFQ6PNZjM6OzthNBq92WWfmThxInbv3u32JyYmBhMmTMDu3buRlZXF8fqOaz+PHDkit0mShMOHDyMkJATJycmIiYnBtGnTcOjQIfkKPABUVlZCo9Fg8eLFXu/3cMSnKvnA1q1b8Ze//AWLFi2Sr667hIaGIi0tDQDwxRdfYPny5YiJiUFOTg6sViv279+Pn/zkJygvL/dF1/1GXl4eWltbYTab5TaOl9PGjRthNpuRnZ2Np556ChcuXMDf//53vPnmm3jllVcAAOfPn8fatWsxb948pKen49atW6ioqEBubi5++9vf+nYHhgmGpw/k5eXhX//6V6/bJkyYgE8++UT++bPPPoPJZMKNGzcQFhaG9PR0FBUV9bo8cTTpLTwBjhfgPFXx3nvv4eOPP8a9e/cwceJE5OfnY/ny5Yq6s2fPoqysDHV1dYiMjMTzzz+P1157DYGBvAnHEwxPIiIVeM6TiEgFhicRkQoMTyIiFRieREQqMDyJiFRgeBIRqcDwJCJSgeFJRKQCw5OISAWGJxGRCv8HclINLywHE8kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'Multi-Targeting'\n",
    "tasks = ['activity']\n",
    "sub_task1 = ['RET','MKNK1','BRAF','SRC','RPS6KB1','TTK','MAPK15','PDPK1','PAK3']\n",
    "sub_task2 = ['AURKA','PAK1','FGFR1','STK11','PAK3','MAP3K7','PIK3CA']\n",
    "kinase_seq_embedding = pd.read_csv('../input/kinase_seq_embedding.csv')\n",
    "smiles_kinase_activity = pd.read_csv('../input/smiles_kinase_activity.csv')\n",
    "print('kinase count:',len(smiles_kinase_activity['kinase'].value_counts()))\n",
    "print('similes count:',len(smiles_kinase_activity['smiles'].value_counts()))\n",
    "\n",
    "smilesList = list(set(smiles_kinase_activity.smiles.values))\n",
    "print(\"number of all smiles: \",len(smilesList))\n",
    "atom_num_dist = []\n",
    "remained_smiles = []\n",
    "canonical_smiles_list = []\n",
    "for smiles in smilesList:\n",
    "    try:        \n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        atom_num_dist.append(len(mol.GetAtoms()))\n",
    "        remained_smiles.append(smiles)\n",
    "        canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "    except:\n",
    "        print(smiles)\n",
    "        pass\n",
    "print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.set(font_scale=1.5)\n",
    "ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EPHA5',\n",
       " 'EPHA8',\n",
       " 'EPHB1',\n",
       " 'EPHB2',\n",
       " 'EPHB3',\n",
       " 'EPHB4',\n",
       " 'EPHB6',\n",
       " 'ABL1',\n",
       " 'TNK2',\n",
       " 'BRSK2',\n",
       " 'WNK2',\n",
       " 'AKT1',\n",
       " 'AKT2',\n",
       " 'AKT3',\n",
       " 'ALK',\n",
       " 'ACVRL1',\n",
       " 'ACVR1',\n",
       " 'BMPR1A',\n",
       " 'ACVR1B',\n",
       " 'TGFBR1',\n",
       " 'BMPR1B',\n",
       " 'DAPK1',\n",
       " 'AURKC',\n",
       " 'AXL',\n",
       " 'GRK2',\n",
       " 'MAP4K2',\n",
       " 'BLK',\n",
       " 'BRAF',\n",
       " 'PTK6',\n",
       " 'BTK',\n",
       " 'CDK7',\n",
       " 'CAMK1',\n",
       " 'EGFR',\n",
       " 'CAMK2A',\n",
       " 'CAMK2B',\n",
       " 'CAMK2G',\n",
       " 'CAMK4',\n",
       " 'CDK1',\n",
       " 'CDC7',\n",
       " 'EPHA4',\n",
       " 'CDK2',\n",
       " 'CDK3',\n",
       " 'CDK4',\n",
       " 'CDK5',\n",
       " 'CDK6',\n",
       " 'PRKG1',\n",
       " 'PRKG2',\n",
       " 'CHEK1',\n",
       " 'CHEK2',\n",
       " 'CSNK1A1',\n",
       " 'CSNK1D',\n",
       " 'JAK1',\n",
       " 'CSNK1E',\n",
       " 'CSNK1G2',\n",
       " 'CSNK1G3',\n",
       " 'CSNK2A1',\n",
       " 'CLK1',\n",
       " 'CLK2',\n",
       " 'CLK3',\n",
       " 'JAK2',\n",
       " 'MAP3K8',\n",
       " 'CSF1R',\n",
       " 'CSK',\n",
       " 'MARK3',\n",
       " 'MAP3K12',\n",
       " 'DMPK',\n",
       " 'PRKDC',\n",
       " 'DYRK1B',\n",
       " 'DYRK2',\n",
       " 'DYRK4',\n",
       " 'EEF2K',\n",
       " 'EIF2AK2',\n",
       " 'MARK2',\n",
       " 'EPHA1',\n",
       " 'EPHA2',\n",
       " 'EPHA3',\n",
       " 'MAPK3',\n",
       " 'MAPK1',\n",
       " 'PTK2',\n",
       " 'FER',\n",
       " 'FES',\n",
       " 'FGFR1',\n",
       " 'FGFR2',\n",
       " 'FGFR3',\n",
       " 'FGFR4',\n",
       " 'FGR',\n",
       " 'FLT3',\n",
       " 'FLT1',\n",
       " 'FLT4',\n",
       " 'MTOR',\n",
       " 'FYN',\n",
       " 'GRK4',\n",
       " 'GRK5',\n",
       " 'GRK6',\n",
       " 'GSK3A',\n",
       " 'JAK3',\n",
       " 'GSK3B',\n",
       " 'HCK',\n",
       " 'ERBB2',\n",
       " 'ERBB4',\n",
       " 'HIPK1',\n",
       " 'IGF1R',\n",
       " 'CHUK',\n",
       " 'IKBKB',\n",
       " 'INSR',\n",
       " 'IRAK1',\n",
       " 'ERN1',\n",
       " 'INSRR',\n",
       " 'ITK',\n",
       " 'MAPK8',\n",
       " 'MAPK9',\n",
       " 'MAPK10',\n",
       " 'MAP4K5',\n",
       " 'ROS1',\n",
       " 'IKBKE',\n",
       " 'NUAK1',\n",
       " 'PIM3',\n",
       " 'KIT',\n",
       " 'CDKL1',\n",
       " 'PLK4',\n",
       " 'LCK',\n",
       " 'LIMK2',\n",
       " 'LTK',\n",
       " 'LYN',\n",
       " 'MAPKAPK2',\n",
       " 'MAPKAPK3',\n",
       " 'MAPKAPK5',\n",
       " 'MARK1',\n",
       " 'MAP2K1',\n",
       " 'DYRK1A',\n",
       " 'MAP2K2',\n",
       " 'MAP2K5',\n",
       " 'MAP2K6',\n",
       " 'MAP3K1',\n",
       " 'MAP3K14',\n",
       " 'MAP3K2',\n",
       " 'MAP3K5',\n",
       " 'MERTK',\n",
       " 'MET',\n",
       " 'MAP2K7',\n",
       " 'MYLK',\n",
       " 'MAP3K9',\n",
       " 'MAP3K10',\n",
       " 'MKNK1',\n",
       " 'MKNK2',\n",
       " 'MAP2K4',\n",
       " 'RPS6KA5',\n",
       " 'RPS6KA4',\n",
       " 'STK3',\n",
       " 'MUSK',\n",
       " 'PKMYT1',\n",
       " 'NEK1',\n",
       " 'NEK2',\n",
       " 'NEK3',\n",
       " 'NEK4',\n",
       " 'IRAK4',\n",
       " 'ROCK2',\n",
       " 'MAPK14',\n",
       " 'RPS6KB1',\n",
       " 'PAK1',\n",
       " 'PAK2',\n",
       " 'PAK3',\n",
       " 'PRKCH',\n",
       " 'NTRK3',\n",
       " 'PDGFRA',\n",
       " 'PDGFRB',\n",
       " 'PDPK1',\n",
       " 'EIF2AK3',\n",
       " 'PHKG2',\n",
       " 'PIM1',\n",
       " 'PIM2',\n",
       " 'CDK9',\n",
       " 'ULK2',\n",
       " 'MELK',\n",
       " 'CDC42BPA',\n",
       " 'PRKACA',\n",
       " 'PRKCA',\n",
       " 'PRKCB',\n",
       " 'PRKCD',\n",
       " 'PRKCE',\n",
       " 'PRKCG',\n",
       " 'PRKCI',\n",
       " 'PRKD1',\n",
       " 'PRKCQ',\n",
       " 'PRKCZ',\n",
       " 'PRKX',\n",
       " 'PLK1',\n",
       " 'PLK3',\n",
       " 'PKN1',\n",
       " 'PKN2',\n",
       " 'RAF1',\n",
       " 'RET',\n",
       " 'GRK1',\n",
       " 'RIPK1',\n",
       " 'RIPK2',\n",
       " 'ROCK1',\n",
       " 'MST1R',\n",
       " 'RPS6KA2',\n",
       " 'RPS6KA3',\n",
       " 'RPS6KA1',\n",
       " 'MAPK11',\n",
       " 'MAPK12',\n",
       " 'MAPK13',\n",
       " 'SGK1',\n",
       " 'SLK',\n",
       " 'PLK2',\n",
       " 'MAP3K11',\n",
       " 'SRC',\n",
       " 'SRPK1',\n",
       " 'SYK',\n",
       " 'BMPR2',\n",
       " 'TEC',\n",
       " 'TEK',\n",
       " 'TESK1',\n",
       " 'TGFBR2',\n",
       " 'TIE1',\n",
       " 'NTRK1',\n",
       " 'NTRK2',\n",
       " 'TTK',\n",
       " 'TXK',\n",
       " 'TYK2',\n",
       " 'TYRO3',\n",
       " 'ULK1',\n",
       " 'VRK1',\n",
       " 'WEE1',\n",
       " 'YES1',\n",
       " 'ZAP70',\n",
       " 'DDR1',\n",
       " 'KDR',\n",
       " 'AURKB',\n",
       " 'AURKA',\n",
       " 'MAPK7',\n",
       " 'DDR2',\n",
       " 'LIMK1',\n",
       " 'EPHA7',\n",
       " 'BMX',\n",
       " 'FRK',\n",
       " 'NEK6',\n",
       " 'AAK1',\n",
       " 'PTK2B',\n",
       " 'PAK6',\n",
       " 'PAK4',\n",
       " 'STK26',\n",
       " 'TAOK3',\n",
       " 'TAOK1',\n",
       " 'MAP4K4',\n",
       " 'TNIK',\n",
       " 'MINK1',\n",
       " 'STK33',\n",
       " 'MAPK15',\n",
       " 'LRRK2',\n",
       " 'TSSK2',\n",
       " 'HASPIN',\n",
       " 'GRK3',\n",
       " 'PRKD2',\n",
       " 'CAMKK2',\n",
       " 'CLK4',\n",
       " 'STK17A',\n",
       " 'STK17B',\n",
       " 'DYRK3',\n",
       " 'PRKD3',\n",
       " 'TNNI3K',\n",
       " 'HIPK2',\n",
       " 'EIF2AK1',\n",
       " 'MAP3K6',\n",
       " 'MAP3K20',\n",
       " 'PAK5',\n",
       " 'SGK2',\n",
       " 'SGK3',\n",
       " 'PBK',\n",
       " 'TBK1',\n",
       " 'GRK7',\n",
       " 'SIK1',\n",
       " 'WNK3',\n",
       " 'EPHA6',\n",
       " 'CSNK1G1',\n",
       " 'MYLK2',\n",
       " 'CAMKK1',\n",
       " 'CAMK2D',\n",
       " 'TSSK1B',\n",
       " 'DAPK3',\n",
       " 'STK10',\n",
       " 'STK11',\n",
       " 'STK16',\n",
       " 'HIPK3',\n",
       " 'HIPK4',\n",
       " 'STK24',\n",
       " 'IRAK3',\n",
       " 'STK4',\n",
       " 'NLK',\n",
       " 'ALPK3',\n",
       " 'STK38L',\n",
       " 'MYO3A',\n",
       " 'DSTYK',\n",
       " 'NUAK2',\n",
       " 'MYLK4',\n",
       " 'MYLK3',\n",
       " 'WEE2',\n",
       " 'SRPK3',\n",
       " 'SRPK2',\n",
       " 'RIOK2',\n",
       " 'RIOK3',\n",
       " 'RIOK1',\n",
       " 'NEK11',\n",
       " 'CDK18',\n",
       " 'CDK19',\n",
       " 'CDK13',\n",
       " 'CDK16',\n",
       " 'CDK17',\n",
       " 'CDK14',\n",
       " 'CDK15',\n",
       " 'TTBK1',\n",
       " 'MAP3K13',\n",
       " 'NEK7',\n",
       " 'NEK5',\n",
       " 'NEK9',\n",
       " 'STK25',\n",
       " 'CDC42BPG',\n",
       " 'CDC42BPB',\n",
       " 'PHKG1',\n",
       " 'STK32C',\n",
       " 'STK32B',\n",
       " 'STK32A',\n",
       " 'MYO3B',\n",
       " 'OXSR1',\n",
       " 'CDKL2',\n",
       " 'CDKL5',\n",
       " 'CDKL3',\n",
       " 'RPS6KB2',\n",
       " 'ACTR2',\n",
       " 'TAOK2',\n",
       " 'CDK8',\n",
       " 'PRKAA2',\n",
       " 'PRKAA1',\n",
       " 'COQ8B',\n",
       " 'COQ8A',\n",
       " 'PRPF4B',\n",
       " 'MAP3K3',\n",
       " 'MAP3K4',\n",
       " 'ERBB3',\n",
       " 'VRK2',\n",
       " 'WNK1',\n",
       " 'PASK',\n",
       " 'ACTR2B',\n",
       " 'TRPM6',\n",
       " 'CDK11A',\n",
       " 'STK35',\n",
       " 'STK36',\n",
       " 'STK38',\n",
       " 'STK39',\n",
       " 'SIK3',\n",
       " 'SIK2',\n",
       " 'BMP2K',\n",
       " 'CASK',\n",
       " 'MARK4',\n",
       " 'TNK1',\n",
       " 'MAST1',\n",
       " 'LATS1',\n",
       " 'MAP4K1',\n",
       " 'MAP4K3',\n",
       " 'CAMK1G',\n",
       " 'MAP2K3',\n",
       " 'GAK',\n",
       " 'ANKK1',\n",
       " 'CSNK2A2',\n",
       " 'PNCK',\n",
       " 'BRSK1',\n",
       " 'HUNK',\n",
       " 'ICK',\n",
       " 'DCLK1',\n",
       " 'DCLK3',\n",
       " 'DCLK2',\n",
       " 'CAMK1D',\n",
       " 'SBK1',\n",
       " 'TLK1',\n",
       " 'TLK2',\n",
       " 'MATK',\n",
       " 'ABL2',\n",
       " 'MAPK6',\n",
       " 'MAPK4',\n",
       " 'CSNK1A1L',\n",
       " 'PRKACG',\n",
       " 'PRKACB',\n",
       " 'SRMS',\n",
       " 'LATS2',\n",
       " 'RIPK4',\n",
       " 'CIT',\n",
       " 'DAPK2',\n",
       " 'ULK3',\n",
       " 'SNRK',\n",
       " 'RPS6KA6',\n",
       " 'MAP3K7']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_tasks = list(smiles_kinase_activity['kinase'].unique())\n",
    "multi_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "random_seed = 28\n",
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "start = time.time()\n",
    "\n",
    "batch_size = 500\n",
    "epochs = 100\n",
    "p_dropout = 0.18\n",
    "fingerprint_dim = 180\n",
    "\n",
    "radius = 3\n",
    "weight_decay = 4.5 # also known as l2_regularization_lambda\n",
    "learning_rate = 2.8\n",
    "embedding_dim = 100\n",
    "output_units_num = 1 * len(multi_tasks) # for classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Standard deviation</th>\n",
       "      <th>Mean absolute deviation</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>activity</td>\n",
       "      <td>5.22636</td>\n",
       "      <td>0.79216</td>\n",
       "      <td>0.572409</td>\n",
       "      <td>1.383907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Task     Mean    ...     Mean absolute deviation     ratio\n",
       "0  activity  5.22636    ...                    0.572409  1.383907\n",
       "\n",
       "[1 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = smiles_kinase_activity.sample(frac=1/10, random_state=random_seed) # test set\n",
    "training_data = smiles_kinase_activity.drop(test_df.index) # training data\n",
    "\n",
    "# get the stats of the seen dataset (the training data)\n",
    "# which will be used to noramlize the dataset. \n",
    "columns = ['Task','Mean','Standard deviation', 'Mean absolute deviation','ratio']\n",
    "mean_list=[]\n",
    "std_list=[]\n",
    "mad_list=[]\n",
    "ratio_list=[]\n",
    "for task in tasks:\n",
    "    mean = training_data[task].mean()\n",
    "    mean_list.append(mean)\n",
    "    std = training_data[task].std()\n",
    "    std_list.append(std)\n",
    "    mad = training_data[task].mad()\n",
    "    mad_list.append(mad)\n",
    "    ratio_list.append(std/mad)\n",
    "    training_data[task+'_normalized'] = (training_data[task]- mean)/std\n",
    "    test_df[task+'_normalized'] = (test_df[task]- mean)/std\n",
    "\n",
    "# training data is further divided into validation set and train set\n",
    "valid_df = training_data.sample(frac=1/9, random_state=random_seed) # validation set\n",
    "train_df = training_data.drop(valid_df.index) # train set\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "valid_df = valid_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "list_of_tuples = list(zip(tasks, mean_list, std_list, mad_list, ratio_list))\n",
    "stats  = pd.DataFrame(list_of_tuples, columns = columns)\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6067693844481934\n",
      "0.7473647274436619\n"
     ]
    }
   ],
   "source": [
    "# predict all values as mean of each ligand will get a MSE of 0.3683609544565635\n",
    "smiles_variance = smiles_kinase_activity.groupby('smiles')['activity'].var(ddof=0) * smiles_kinase_activity.groupby('smiles')['smiles'].value_counts()\n",
    "print(np.sqrt(smiles_variance.sum()/smiles_kinase_activity.shape[0]))\n",
    "\n",
    "# predict all values as mean of each protein kinase will get a MSE of 0.5583972950594606\n",
    "kinase_variance = smiles_kinase_activity.groupby('kinase')['activity'].var(ddof=0) * smiles_kinase_activity.groupby('kinase')['kinase'].value_counts()\n",
    "print(np.sqrt(kinase_variance.sum()/smiles_kinase_activity.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6055113264956725\n",
      "0.7465290722867107\n"
     ]
    }
   ],
   "source": [
    "# predict all values as mean of each ligand will get a MSE of 0.3683609544565635\n",
    "smiles_variance = training_data.groupby('smiles')['activity'].var(ddof=0) * training_data.groupby('smiles')['smiles'].value_counts()\n",
    "print(np.sqrt(smiles_variance.sum()/training_data.shape[0]))\n",
    "\n",
    "# predict all values as mean of each protein kinase will get a MSE of 0.5583972950594606\n",
    "kinase_variance = training_data.groupby('kinase')['activity'].var(ddof=0) * training_data.groupby('kinase')['kinase'].value_counts()\n",
    "print(np.sqrt(kinase_variance.sum()/training_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dicts = get_smiles_dicts(smilesList)\n",
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([smilesList[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "model = LigandBased(radius, num_atom_features, num_bond_features,\\\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), 10**-learning_rate, weight_decay=10**-weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "809312\n",
      "atom_fc.weight torch.Size([180, 39])\n",
      "atom_fc.bias torch.Size([180])\n",
      "neighbor_fc.weight torch.Size([180, 49])\n",
      "neighbor_fc.bias torch.Size([180])\n",
      "atom_GRU.0.weight_ih torch.Size([540, 180])\n",
      "atom_GRU.0.weight_hh torch.Size([540, 180])\n",
      "atom_GRU.0.bias_ih torch.Size([540])\n",
      "atom_GRU.0.bias_hh torch.Size([540])\n",
      "atom_GRU.1.weight_ih torch.Size([540, 180])\n",
      "atom_GRU.1.weight_hh torch.Size([540, 180])\n",
      "atom_GRU.1.bias_ih torch.Size([540])\n",
      "atom_GRU.1.bias_hh torch.Size([540])\n",
      "atom_GRU.2.weight_ih torch.Size([540, 180])\n",
      "atom_GRU.2.weight_hh torch.Size([540, 180])\n",
      "atom_GRU.2.bias_ih torch.Size([540])\n",
      "atom_GRU.2.bias_hh torch.Size([540])\n",
      "hidden.weight torch.Size([360, 180])\n",
      "hidden.bias torch.Size([360])\n",
      "output.weight torch.Size([392, 360])\n",
      "output.bias torch.Size([392])\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, optimizer, loss_function):\n",
    "    model.train()\n",
    "    np.random.seed(epoch)\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)   \n",
    "    for counter, train_batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[train_batch,:]\n",
    "        smiles_list = batch_df.smiles.values\n",
    "        y_val = batch_df['activity_normalized'].values\n",
    "        tasks_mask = torch.cuda.ByteTensor([np.array([x == task for task in multi_tasks]) for x in batch_df['kinase']])\n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        mol_prediction = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\\\n",
    "                                                 torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask))\n",
    "        model.zero_grad()\n",
    "        mol_prediction = torch.masked_select(mol_prediction, tasks_mask)\n",
    "        # Compute your loss function. (Again, Torch wants the target wrapped in a variable)\n",
    "        loss = loss_function(mol_prediction, torch.Tensor(y_val))\n",
    "        # Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "def eval(model, dataset):\n",
    "    model.eval()\n",
    "    eval_MAE_list = []\n",
    "    eval_MSE_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)   \n",
    "    for counter, train_batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[train_batch,:]\n",
    "        smiles_list = batch_df.smiles.values\n",
    "        y_val = batch_df['activity_normalized'].values\n",
    "        \n",
    "        tasks_mask = torch.cuda.ByteTensor([np.array([x == task for task in multi_tasks]) for x in batch_df['kinase']])\n",
    "\n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        mol_prediction = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\\\n",
    "                                                 torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask))\n",
    "\n",
    "        mol_prediction = torch.masked_select(mol_prediction, tasks_mask)\n",
    "        MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val), reduction='none')\n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val), reduction='none')\n",
    "        eval_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        eval_MSE_list.extend(MSE.data.squeeze().cpu().numpy())\n",
    "    eval_MAE_nomalized = np.array(eval_MAE_list).mean()\n",
    "    eval_MSE_nomalized = np.array(eval_MSE_list).mean()\n",
    "    eval_MAE = eval_MAE_nomalized * std_list[0]\n",
    "    eval_MSE = eval_MSE_nomalized * std_list[0] * std_list[0]\n",
    "    return eval_MAE, eval_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9536241913759269 0.956613629862973\n",
      "CPU times: user 3min 25s, sys: 2min 30s, total: 5min 56s\n",
      "Wall time: 5min 56s\n",
      "1 0.7133326555677681 0.7233447184163486\n",
      "CPU times: user 3min 25s, sys: 2min 30s, total: 5min 56s\n",
      "Wall time: 5min 56s\n",
      "2 0.6377057436480472 0.6515253671044201\n",
      "CPU times: user 3min 24s, sys: 2min 31s, total: 5min 55s\n",
      "Wall time: 5min 55s\n",
      "3 0.6081096215479306 0.6229367495611846\n",
      "CPU times: user 3min 21s, sys: 2min 32s, total: 5min 53s\n",
      "Wall time: 5min 54s\n",
      "4 0.5752693479875884 0.5914621252121979\n",
      "CPU times: user 3min 21s, sys: 2min 33s, total: 5min 54s\n",
      "Wall time: 5min 55s\n",
      "5 0.5641677451764812 0.5816439155629329\n",
      "CPU times: user 3min 21s, sys: 2min 33s, total: 5min 54s\n",
      "Wall time: 5min 55s\n",
      "6 0.5566060445772274 0.5758131633842535\n",
      "CPU times: user 3min 19s, sys: 2min 34s, total: 5min 54s\n",
      "Wall time: 5min 54s\n",
      "7 0.5600872362607564 0.5793824231181218\n",
      "CPU times: user 3min 18s, sys: 2min 34s, total: 5min 53s\n",
      "Wall time: 5min 53s\n",
      "8 0.5421865477891409 0.5637343899938798\n",
      "CPU times: user 3min 19s, sys: 2min 34s, total: 5min 53s\n",
      "Wall time: 5min 54s\n",
      "9 0.5413906475253177 0.5633653710026444\n",
      "CPU times: user 3min 21s, sys: 2min 33s, total: 5min 55s\n",
      "Wall time: 5min 55s\n",
      "10 0.5355822550689571 0.5594700935193541\n",
      "CPU times: user 3min 21s, sys: 2min 34s, total: 5min 55s\n",
      "Wall time: 5min 56s\n",
      "11 0.5316965145064968 0.5555269864907644\n",
      "CPU times: user 3min 23s, sys: 2min 32s, total: 5min 56s\n",
      "Wall time: 5min 56s\n",
      "12 0.5377941812240556 0.5614009237466158\n",
      "CPU times: user 3min 25s, sys: 2min 30s, total: 5min 56s\n",
      "Wall time: 5min 56s\n",
      "13 0.534974807733139 0.5605520870761156\n",
      "CPU times: user 3min 26s, sys: 2min 29s, total: 5min 56s\n",
      "Wall time: 5min 56s\n",
      "14 0.5209504218616289 0.546707323109862\n",
      "CPU times: user 3min 23s, sys: 2min 30s, total: 5min 54s\n",
      "Wall time: 5min 55s\n",
      "15 0.5267873905576995 0.5527867614844506\n",
      "CPU times: user 3min 27s, sys: 2min 29s, total: 5min 57s\n",
      "Wall time: 5min 57s\n",
      "16 0.5259494145806848 0.5509082161201512\n",
      "CPU times: user 3min 29s, sys: 2min 29s, total: 5min 58s\n",
      "Wall time: 5min 59s\n",
      "17 0.5240262865876681 0.5506830867954436\n",
      "CPU times: user 3min 30s, sys: 2min 28s, total: 5min 59s\n",
      "Wall time: 5min 59s\n",
      "18 0.5130365336193412 0.541061434837167\n",
      "CPU times: user 3min 29s, sys: 2min 29s, total: 5min 59s\n",
      "Wall time: 5min 59s\n",
      "19 0.5129558213088685 0.5418203873094302\n",
      "CPU times: user 3min 27s, sys: 2min 30s, total: 5min 58s\n",
      "Wall time: 5min 58s\n",
      "20 0.4994572449094537 0.5307637635455154\n",
      "CPU times: user 3min 27s, sys: 2min 30s, total: 5min 57s\n",
      "Wall time: 5min 57s\n",
      "21 0.5002829620523506 0.5302289253462509\n",
      "CPU times: user 3min 26s, sys: 2min 30s, total: 5min 57s\n",
      "Wall time: 5min 58s\n",
      "22 0.5190140678382545 0.546409482751284\n",
      "CPU times: user 3min 26s, sys: 2min 30s, total: 5min 57s\n",
      "Wall time: 5min 57s\n",
      "23 0.5033283678814925 0.532229930633366\n",
      "CPU times: user 3min 27s, sys: 2min 30s, total: 5min 57s\n",
      "Wall time: 5min 58s\n",
      "24 0.4912074535142076 0.5235910502675915\n",
      "CPU times: user 3min 28s, sys: 2min 29s, total: 5min 57s\n",
      "Wall time: 5min 58s\n",
      "25 0.49141031898790294 0.5276171238890246\n",
      "CPU times: user 3min 26s, sys: 2min 31s, total: 5min 58s\n",
      "Wall time: 5min 58s\n",
      "26 0.49493435656231677 0.5281349545165817\n",
      "CPU times: user 3min 25s, sys: 2min 32s, total: 5min 57s\n",
      "Wall time: 5min 58s\n",
      "27 0.4934245701552856 0.5264527048068376\n",
      "CPU times: user 3min 24s, sys: 2min 34s, total: 5min 58s\n",
      "Wall time: 5min 58s\n",
      "28 0.4908021378319647 0.5256427832147651\n",
      "CPU times: user 3min 24s, sys: 2min 33s, total: 5min 58s\n",
      "Wall time: 5min 58s\n",
      "29 0.49049620123327353 0.5235134834307313\n",
      "CPU times: user 3min 24s, sys: 2min 34s, total: 5min 58s\n",
      "Wall time: 5min 58s\n",
      "30 0.4906413128699635 0.5286137345823179\n",
      "CPU times: user 3min 26s, sys: 2min 32s, total: 5min 59s\n",
      "Wall time: 5min 59s\n",
      "31 0.4868560822218983 0.5235483122370131\n",
      "CPU times: user 3min 26s, sys: 2min 32s, total: 5min 59s\n",
      "Wall time: 5min 59s\n",
      "32 0.48883963378519096 0.5249362207576187\n",
      "CPU times: user 3min 26s, sys: 2min 33s, total: 6min\n",
      "Wall time: 6min\n",
      "33 0.4876328986572469 0.5246644648568072\n",
      "CPU times: user 3min 26s, sys: 2min 32s, total: 5min 58s\n",
      "Wall time: 5min 59s\n",
      "34 0.4708582751583758 0.5104745572600177\n",
      "CPU times: user 3min 22s, sys: 2min 34s, total: 5min 56s\n",
      "Wall time: 5min 57s\n",
      "35 0.48504362042484334 0.5238967763749645\n",
      "CPU times: user 3min 20s, sys: 2min 34s, total: 5min 55s\n",
      "Wall time: 5min 56s\n",
      "36 0.47636622797291717 0.5158903226790101\n",
      "CPU times: user 3min 19s, sys: 2min 35s, total: 5min 54s\n",
      "Wall time: 5min 55s\n",
      "37 0.47501270836938925 0.5147643369080077\n",
      "CPU times: user 3min 20s, sys: 2min 34s, total: 5min 54s\n",
      "Wall time: 5min 54s\n",
      "38 0.4781555742195203 0.5162697664129036\n",
      "CPU times: user 3min 18s, sys: 2min 34s, total: 5min 53s\n",
      "Wall time: 5min 54s\n",
      "39 0.48158054484496104 0.5207623859249418\n",
      "CPU times: user 3min 18s, sys: 2min 34s, total: 5min 53s\n",
      "Wall time: 5min 54s\n",
      "40 0.4769347073065583 0.5166497459182817\n",
      "CPU times: user 3min 20s, sys: 2min 33s, total: 5min 53s\n",
      "Wall time: 5min 54s\n",
      "41 0.478960949680502 0.519263570552409\n",
      "CPU times: user 3min 18s, sys: 2min 34s, total: 5min 53s\n",
      "Wall time: 5min 54s\n",
      "42 0.47256323602764216 0.5148741514024745\n",
      "CPU times: user 3min 20s, sys: 2min 32s, total: 5min 53s\n",
      "Wall time: 5min 54s\n",
      "43 0.46341075850460656 0.5073512407840716\n",
      "CPU times: user 3min 20s, sys: 2min 33s, total: 5min 53s\n",
      "Wall time: 5min 53s\n",
      "44 0.4747678591058748 0.5170970815898491\n",
      "CPU times: user 3min 21s, sys: 2min 31s, total: 5min 53s\n",
      "Wall time: 5min 53s\n",
      "45 0.4707239102501296 0.5135354905314006\n",
      "CPU times: user 3min 19s, sys: 2min 33s, total: 5min 53s\n",
      "Wall time: 5min 53s\n",
      "46 0.475539783956038 0.5187866154424856\n",
      "CPU times: user 3min 20s, sys: 2min 32s, total: 5min 53s\n",
      "Wall time: 5min 53s\n",
      "47 0.4653516003602009 0.5093179279284658\n",
      "CPU times: user 3min 20s, sys: 2min 32s, total: 5min 52s\n",
      "Wall time: 5min 53s\n",
      "48 0.4683548348266646 0.5135460878096331\n",
      "CPU times: user 3min 21s, sys: 2min 31s, total: 5min 52s\n",
      "Wall time: 5min 53s\n",
      "49 0.4658763992601139 0.509604565260465\n",
      "CPU times: user 3min 22s, sys: 2min 30s, total: 5min 52s\n",
      "Wall time: 5min 53s\n",
      "50 0.47669510191027636 0.5187247885337184\n",
      "CPU times: user 3min 21s, sys: 2min 31s, total: 5min 52s\n",
      "Wall time: 5min 53s\n",
      "51 0.4613312193183926 0.5063106739806421\n",
      "CPU times: user 3min 23s, sys: 2min 29s, total: 5min 52s\n",
      "Wall time: 5min 53s\n",
      "52 0.4664129374662315 0.512426191702865\n",
      "CPU times: user 3min 22s, sys: 2min 30s, total: 5min 52s\n",
      "Wall time: 5min 53s\n",
      "53 0.46245902967089775 0.5087749176778981\n",
      "CPU times: user 3min 26s, sys: 2min 26s, total: 5min 52s\n",
      "Wall time: 5min 53s\n",
      "54 0.4579907236598171 0.5056143058788054\n",
      "CPU times: user 3min 23s, sys: 2min 29s, total: 5min 53s\n",
      "Wall time: 5min 53s\n",
      "55 0.46183133034622753 0.5089788096087289\n",
      "CPU times: user 3min 24s, sys: 2min 28s, total: 5min 52s\n",
      "Wall time: 5min 53s\n",
      "56 0.4603948725728711 0.5081806365773828\n",
      "CPU times: user 3min 24s, sys: 2min 28s, total: 5min 52s\n",
      "Wall time: 5min 53s\n",
      "57 0.4644476404640588 0.5118345606220857\n",
      "CPU times: user 3min 21s, sys: 2min 31s, total: 5min 52s\n",
      "Wall time: 5min 53s\n",
      "58 0.461661122075463 0.5100208080355246\n",
      "CPU times: user 3min 22s, sys: 2min 29s, total: 5min 52s\n",
      "Wall time: 5min 53s\n",
      "59 0.45764577287862046 0.5079484804115468\n",
      "CPU times: user 3min 20s, sys: 2min 32s, total: 5min 52s\n",
      "Wall time: 5min 53s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(60):\n",
    "    valid_MAE, valid_MSE = eval(model, valid_df)\n",
    "    train_MAE, train_MSE = eval(model, train_df)\n",
    "    print(epoch, np.sqrt(train_MSE), np.sqrt(valid_MSE))\n",
    "    \n",
    "    %time train(model, train_df, optimizer, loss_function)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
